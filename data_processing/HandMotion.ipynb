{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import argrelextrema\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_svc_files(root_dir):\n",
    "    svc_files = []\n",
    "    \n",
    "    # Walk through all directories and subdirectories\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\"_1_1.svc\"):  # Check if the file has .svc extension\n",
    "                svc_files.append(os.path.join(dirpath, file))  # Store full path\n",
    "    \n",
    "    return svc_files\n",
    "\n",
    "def feature_extraction_HandMotion(folder_path, label):\n",
    "    txt_files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "    dataframes = []\n",
    "\n",
    "    for txt_file in txt_files:\n",
    "        txt_path = os.path.join(folder_path, txt_file)\n",
    "        \n",
    "        df = pd.read_csv(txt_path, delimiter=\";\", names=[\"X\", \"Y\", \"Z\", \"Pressure\", \"GripAngle\", \"Timestamp\", \"Test ID\"])\n",
    "        df = df[df[\"Pressure\"] != 0]\n",
    "        # Compute delta time\n",
    "        df[\"dt\"] = df[\"Timestamp\"].diff()\n",
    "        df[\"dt\"] = df[\"dt\"].replace(0, np.nan).interpolate().ffill().bfill()  # No zeros!\n",
    "          # Avoid division by zero\n",
    "        # Compute displacement\n",
    "        df[\"dx\"] = df[\"X\"].diff().fillna(0)\n",
    "        df[\"dy\"] = df[\"Y\"].diff().fillna(0)\n",
    "        # Compute trajectory length\n",
    "        df[\"trajectory_length\"] = np.sqrt(df[\"dx\"]**2 + df[\"dy\"]**2)\n",
    "        # Stroke speed\n",
    "        stroke_speed = df[\"trajectory_length\"].sum() / (df[\"Timestamp\"].iloc[-1] - df[\"Timestamp\"].iloc[0])\n",
    "        # Compute velocity\n",
    "        df[\"velocity\"] = df[\"trajectory_length\"] / df[\"dt\"]\n",
    "        df[\"velocity\"] = df[\"velocity\"].replace([np.inf, -np.inf], np.nan).interpolate().ffill().bfill()\n",
    "        # Compute acceleration\n",
    "        df[\"acceleration\"] = df[\"velocity\"].diff().fillna(0) / df[\"dt\"]\n",
    "        df[\"acceleration\"] = df[\"acceleration\"].replace([np.inf, -np.inf], np.nan).interpolate().ffill().bfill()\n",
    "        # Compute jerk\n",
    "        df[\"jerk\"] = df[\"acceleration\"].diff().fillna(0) / df[\"dt\"]\n",
    "        df[\"jerk\"] = df[\"jerk\"].replace([np.inf, -np.inf], np.nan).interpolate().ffill().bfill()\n",
    "        # Number of changes in velocity and acceleration direction\n",
    "        ncv = len(argrelextrema(df[\"velocity\"].values, np.less)[0]) + len(argrelextrema(df[\"velocity\"].values, np.greater)[0])\n",
    "        nca = len(argrelextrema(df[\"acceleration\"].values, np.less)[0]) + len(argrelextrema(df[\"acceleration\"].values, np.greater)[0])\n",
    "        # Path efficiency\n",
    "        euclidean_distance = np.sqrt((df[\"X\"].iloc[-1] - df[\"X\"].iloc[0])**2 + (df[\"Y\"].iloc[-1] - df[\"Y\"].iloc[0])**2)\n",
    "        path_efficiency = euclidean_distance / df[\"trajectory_length\"].sum()\n",
    "        # Stroke duration\n",
    "        stroke_duration = df[\"Timestamp\"].iloc[-1] - df[\"Timestamp\"].iloc[0]\n",
    "        # Pressure features\n",
    "        mean_pressure = df[\"Pressure\"].mean()\n",
    "        std_pressure = df[\"Pressure\"].std()\n",
    "        num_pressure_drops = sum(df[\"Pressure\"].diff().fillna(0) < -0.1)\n",
    "\n",
    "        # Feature DataFrame\n",
    "        feature_data = pd.DataFrame([[\n",
    "            stroke_speed, df[\"velocity\"].mean(), df[\"acceleration\"].mean(), df[\"jerk\"].mean(), \n",
    "            ncv, nca, path_efficiency, stroke_duration, mean_pressure, std_pressure, num_pressure_drops, label\n",
    "        ]], columns=[\n",
    "            \"stroke_speed\", \"mean_velocity\", \"mean_acceleration\", \"mean_jerk\", \n",
    "            \"NCV\", \"NCA\", \"path_efficiency\", \"stroke_duration\",\n",
    "            \"mean_pressure\", \"std_pressure\", \"num_pressure_drops\", \"label\"\n",
    "        ])\n",
    "\n",
    "        dataframes.append(feature_data)\n",
    "\n",
    "    # Combine all DataFrames\n",
    "    if dataframes:\n",
    "        return pd.concat(dataframes, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "columns = [\"Y\", \"X\", \"Timestamp\", \"button state\", \"azimuth\", \"altitude\", \"Pressure\"]\n",
    "dataframes_PaHaW = []\n",
    "\n",
    "\n",
    "def feature_extraction_HandMotion_ext(folder_path, label):\n",
    "    \n",
    "    svc_files_list = find_svc_files(folder_path)\n",
    "\n",
    "    for file_path in svc_files_list:\n",
    "        df = pd.read_csv(file_path, delimiter=\" \", header=None, names=columns, skiprows=1)\n",
    "\n",
    "        df = df[df[\"Pressure\"] != 0]\n",
    "        # Compute delta time\n",
    "        df[\"dt\"] = df[\"Timestamp\"].diff()\n",
    "        df[\"dt\"] = df[\"dt\"].replace(0, np.nan).interpolate().ffill().bfill()  # No zeros!\n",
    "          # Avoid division by zero\n",
    "        # Compute displacement\n",
    "        df[\"dx\"] = df[\"X\"].diff().fillna(0)\n",
    "        df[\"dy\"] = df[\"Y\"].diff().fillna(0)\n",
    "        # Compute trajectory length\n",
    "        df[\"trajectory_length\"] = np.sqrt(df[\"dx\"]**2 + df[\"dy\"]**2)\n",
    "        # Stroke speed\n",
    "        stroke_speed = df[\"trajectory_length\"].sum() / (df[\"Timestamp\"].iloc[-1] - df[\"Timestamp\"].iloc[0])\n",
    "        # Compute velocity\n",
    "        df[\"velocity\"] = df[\"trajectory_length\"] / df[\"dt\"]\n",
    "        df[\"velocity\"] = df[\"velocity\"].replace([np.inf, -np.inf], np.nan).interpolate().ffill().bfill()\n",
    "        # Compute acceleration\n",
    "        df[\"acceleration\"] = df[\"velocity\"].diff().fillna(0) / df[\"dt\"]\n",
    "        df[\"acceleration\"] = df[\"acceleration\"].replace([np.inf, -np.inf], np.nan).interpolate().ffill().bfill()\n",
    "        # Compute jerk\n",
    "        df[\"jerk\"] = df[\"acceleration\"].diff().fillna(0) / df[\"dt\"]\n",
    "        df[\"jerk\"] = df[\"jerk\"].replace([np.inf, -np.inf], np.nan).interpolate().ffill().bfill()\n",
    "        # Number of changes in velocity and acceleration direction\n",
    "        ncv = len(argrelextrema(df[\"velocity\"].values, np.less)[0]) + len(argrelextrema(df[\"velocity\"].values, np.greater)[0])\n",
    "        nca = len(argrelextrema(df[\"acceleration\"].values, np.less)[0]) + len(argrelextrema(df[\"acceleration\"].values, np.greater)[0])\n",
    "        # Path efficiency\n",
    "        euclidean_distance = np.sqrt((df[\"X\"].iloc[-1] - df[\"X\"].iloc[0])**2 + (df[\"Y\"].iloc[-1] - df[\"Y\"].iloc[0])**2)\n",
    "        path_efficiency = euclidean_distance / df[\"trajectory_length\"].sum()\n",
    "        # Stroke duration\n",
    "        stroke_duration = df[\"Timestamp\"].iloc[-1] - df[\"Timestamp\"].iloc[0]\n",
    "        # Pressure features\n",
    "        mean_pressure = df[\"Pressure\"].mean()\n",
    "        std_pressure = df[\"Pressure\"].std()\n",
    "        num_pressure_drops = sum(df[\"Pressure\"].diff().fillna(0) < -0.1)\n",
    "\n",
    "        \n",
    "\n",
    "        # Feature DataFrame\n",
    "        feature_data = pd.DataFrame([[\n",
    "            stroke_speed, df[\"velocity\"].mean(), df[\"acceleration\"].mean(), df[\"jerk\"].mean(), \n",
    "            ncv, nca, path_efficiency, stroke_duration, mean_pressure, std_pressure, num_pressure_drops, label\n",
    "        ]], columns=[\n",
    "            \"stroke_speed\", \"mean_velocity\", \"mean_acceleration\", \"mean_jerk\", \n",
    "            \"NCV\", \"NCA\", \"path_efficiency\", \"stroke_duration\",\n",
    "            \"mean_pressure\", \"std_pressure\", \"num_pressure_drops\", \"label\"\n",
    "        ])\n",
    "\n",
    "        dataframes_PaHaW.append(feature_data)\n",
    "\n",
    "    # Combine all DataFrames\n",
    "    if dataframes_PaHaW:\n",
    "        return pd.concat(dataframes_PaHaW, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00005/00005__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00006/00006__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00019/00019__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00078/00078__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00033/00033__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00024/00024__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00018/00018__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00004/00004__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00075/00075__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00002/00002__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00034/00034__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00017/00017__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00010/00010__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00007/00007__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00036/00036__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00016/00016__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00009/00009__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00015/00015__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00013/00013__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00098/00098__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00025/00025__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00044/00044__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00043/00043__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00055/00055__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00022/00022__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00008/00008__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00077/00077__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00048/00048__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00001/00001__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00074/00074__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00014/00014__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00020/00020__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00023/00023__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00054/00054__1_1.svc',\n",
       " '/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD/00003/00003__1_1.svc']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_svc_files(\"/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load UCI (Istanbul) dataset\n",
    "# folder_path_HC = \"/home/varsallz/Diagnostic_Aid_PD/data/Drawing/ISTANBUL/Improved Spiral Test Using Digitized Graphics Tablet for Monitoring Parkinson's Disease/data/Healthy\"\n",
    "# df_HC = feature_extraction_HandMotion(folder_path_HC, label=0)\n",
    "\n",
    "# folder_path_PD = \"/home/varsallz/Diagnostic_Aid_PD/data/Drawing/ISTANBUL/Improved Spiral Test Using Digitized Graphics Tablet for Monitoring Parkinson's Disease/data/PWP\"\n",
    "# df_PD = feature_extraction_HandMotion(folder_path_PD, label=1)\n",
    "\n",
    "# df = pd.concat([df_PD, df_HC], axis=0, ignore_index=True)\n",
    "# df = df.sample(frac=1).reset_index(drop=True)\n",
    "# df = df.dropna()\n",
    "# df.to_csv(\"HandMotionFeatures.csv\", index=False)\n",
    "\n",
    "\n",
    "# Load PaHaW dataset\n",
    "HC1_folder_path = \"/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/HC\"\n",
    "df_HC1 = feature_extraction_HandMotion_ext(HC1_folder_path, label=0)\n",
    "df_HC1 = df_HC1.replace([np.inf, -np.inf], np.nan).ffill(axis=1)\n",
    "\n",
    "PD1_folder_path = \"/home/varsallz/Diagnostic_Aid_PD/data/Drawing/PaHaW/PaHaW/PaHaW_public/PD\"\n",
    "df_PD1= feature_extraction_HandMotion_ext(PD1_folder_path, label=1)\n",
    "df_PD1 = df_PD1.replace([np.inf, -np.inf], np.nan).ffill(axis=1)\n",
    "\n",
    "df1 = pd.concat([df_PD1, df_HC1], axis=0)\n",
    "df1 = df1.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "output_csv = \"PaHaW_features_1.csv\"\n",
    "df1.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for RandomForest: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Best parameters for SVM: {'C': 10, 'kernel': 'rbf'}\n",
      "Best parameters for LogisticRegression: {'C': 1}\n",
      "Best parameters for XGBoost: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 200}\n",
      "                Model  Accuracy  Precision    Recall  F1-Score\n",
      "0        RandomForest  0.803571   0.791667  0.527778  0.633333\n",
      "1                 SVM  0.696429   0.525000  0.583333  0.552632\n",
      "2  LogisticRegression  0.638393   0.455446  0.638889  0.531792\n",
      "3             XGBoost  0.834821   0.843137  0.597222  0.699187\n"
     ]
    }
   ],
   "source": [
    "# **Feature Scaling**\n",
    "X = df1.drop(columns=[\"label\"])\n",
    "y = df1[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Standardize Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=0.95)  # Retain 95% variance\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Handle Class Imbalance with SMOTE \n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_pca, y_train)\n",
    "\n",
    "#######################################################################################################\n",
    "\n",
    "# **Model Training and Evaluation**\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=500, solver = 'lbfgs'),\n",
    "    \"XGBoost\": XGBClassifier()\n",
    "}\n",
    "param_grids = {\n",
    "    \"RandomForest\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_depth\": [None, 10, 20],\n",
    "        \"min_samples_split\": [2, 5, 10]\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": [0.1, 1, 10],\n",
    "        \"kernel\": [\"linear\", \"rbf\"]\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"C\": [0.01, 0.1, 1, 10]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"n_estimators\": [100, 200, 300],\n",
    "        \"max_depth\": [3, 5, 7],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.2]\n",
    "    }\n",
    "}\n",
    "results = []\n",
    "best_models = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    grid_search = GridSearchCV(model, param_grids[model_name], cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "    grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "    best_models[model_name] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "    y_pred = best_models[model_name].predict(X_test_pca)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    results.append([model_name, acc, precision, recall, f1])\n",
    "\n",
    "    # Save trained models\n",
    "    model_path = f\"trained_models/{model_name}.pkl\"\n",
    "    os.makedirs(\"trained_models\", exist_ok=True)\n",
    "    pd.to_pickle(best_models[model_name], model_path)\n",
    "\n",
    "        # Save Scaler & PCA\n",
    "    os.makedirs(\"saved_models\", exist_ok=True)\n",
    "    with open(\"saved_models/scaler.pkl\", \"wb\") as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    with open(\"saved_models/pca.pkl\", \"wb\") as f:\n",
    "        pickle.dump(pca, f)\n",
    "\n",
    "\n",
    "# **Save results**\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "results_df.to_csv(\"ModelPerformance_with_PaHaw.csv\", index=False)\n",
    "\n",
    "# Display results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for RandomForest: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best parameters for SVM: {'C': 1, 'kernel': 'linear'}\n",
      "Best parameters for LogisticRegression: {'C': 1}\n",
      "Best parameters for XGBoost: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}\n",
      "                Model  Accuracy  Precision    Recall  F1-Score\n",
      "0        RandomForest       0.8   0.833333  0.833333  0.833333\n",
      "1                 SVM       1.0   1.000000  1.000000  1.000000\n",
      "2  LogisticRegression       1.0   1.000000  1.000000  1.000000\n",
      "3             XGBoost       1.0   1.000000  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "# # **Feature Scaling**\n",
    "# X = df.drop(columns=[\"label\"])\n",
    "# y = df[\"label\"]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# # Standardize Data\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# # Apply PCA\n",
    "# pca = PCA(n_components=0.95)  # Retain 95% variance\n",
    "# X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "# X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "# # Handle Class Imbalance with SMOTE \n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train_balanced, y_train_balanced = smote.fit_resample(X_train_pca, y_train)\n",
    "\n",
    "# #######################################################################################################\n",
    "\n",
    "# # **Model Training and Evaluation**\n",
    "# models = {\n",
    "#     \"RandomForest\": RandomForestClassifier(),\n",
    "#     \"SVM\": SVC(),\n",
    "#     \"LogisticRegression\": LogisticRegression(max_iter=500, solver = 'lbfgs'),\n",
    "#     \"XGBoost\": XGBClassifier()\n",
    "# }\n",
    "# param_grids = {\n",
    "#     \"RandomForest\": {\n",
    "#         \"n_estimators\": [100, 200, 300],\n",
    "#         \"max_depth\": [None, 10, 20],\n",
    "#         \"min_samples_split\": [2, 5, 10]\n",
    "#     },\n",
    "#     \"SVM\": {\n",
    "#         \"C\": [0.1, 1, 10],\n",
    "#         \"kernel\": [\"linear\", \"rbf\"]\n",
    "#     },\n",
    "#     \"LogisticRegression\": {\n",
    "#         \"C\": [0.01, 0.1, 1, 10]\n",
    "#     },\n",
    "#     \"XGBoost\": {\n",
    "#         \"n_estimators\": [100, 200, 300],\n",
    "#         \"max_depth\": [3, 5, 7],\n",
    "#         \"learning_rate\": [0.01, 0.1, 0.2]\n",
    "#     }\n",
    "# }\n",
    "# results = []\n",
    "# best_models = {}\n",
    "\n",
    "# for model_name, model in models.items():\n",
    "#     grid_search = GridSearchCV(model, param_grids[model_name], cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "#     grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "#     best_models[model_name] = grid_search.best_estimator_\n",
    "#     print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "\n",
    "#     # best_models[model_name].fit(X_train_balanced, y_train_balanced)\n",
    "#     y_pred = best_models[model_name].predict(X_test_pca)\n",
    "\n",
    "#     acc = accuracy_score(y_test, y_pred)\n",
    "#     precision = precision_score(y_test, y_pred)\n",
    "#     recall = recall_score(y_test, y_pred)\n",
    "#     f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "#     results.append([model_name, acc, precision, recall, f1])\n",
    "\n",
    "#     # Save trained models\n",
    "#     model_path = f\"trained_models/{model_name}.pkl\"\n",
    "#     os.makedirs(\"trained_models\", exist_ok=True)\n",
    "#     pd.to_pickle(best_models[model_name], model_path)\n",
    "\n",
    "#     # Save Scaler & PCA\n",
    "#     os.makedirs(\"saved_models\", exist_ok=True)\n",
    "#     with open(\"saved_models/scaler.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(scaler, f)\n",
    "\n",
    "#     with open(\"saved_models/pca.pkl\", \"wb\") as f:\n",
    "#         pickle.dump(pca, f)\n",
    "\n",
    "# results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "# results_df.to_csv(\"ModelPerformance_UCI.csv\", index=False)\n",
    "# print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text on external dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model  Accuracy  Precision    Recall  F1-Score\n",
      "0        RandomForest  0.421053   0.545455  0.260870  0.352941\n",
      "1                 SVM  0.684211   0.720000  0.782609  0.750000\n",
      "2  LogisticRegression  0.289474   0.333333  0.173913  0.228571\n",
      "3             XGBoost  0.421053   0.545455  0.260870  0.352941\n"
     ]
    }
   ],
   "source": [
    "# Load new dataset\n",
    "new_df = pd.read_csv(\"HandMotionFeatures.csv\")  # Replace with actual dataset path\n",
    "\n",
    "if \"label\" in new_df.columns:\n",
    "    X_new = new_df.drop(columns=[\"label\"])\n",
    "    y_new = new_df[\"label\"]\n",
    "else:\n",
    "    X_new = new_df  # No labels, only features\n",
    "\n",
    "# Load previously fitted scaler & PCA (assuming they were saved separately)\n",
    "scaler = pd.read_pickle(\"saved_models/scaler.pkl\")\n",
    "pca = pd.read_pickle(\"saved_models/pca.pkl\")\n",
    "\n",
    "# Apply same preprocessing steps\n",
    "X_new_scaled = scaler.transform(X_new)  \n",
    "X_new_pca = pca.transform(X_new_scaled)  \n",
    "\n",
    "# Load trained models\n",
    "model_names = [\"RandomForest\", \"SVM\", \"LogisticRegression\", \"XGBoost\"]\n",
    "models = {}\n",
    "\n",
    "for name in model_names:\n",
    "    model_path = f\"trained_models/{name}.pkl\"\n",
    "    if os.path.exists(model_path):\n",
    "        models[name] = pd.read_pickle(model_path)\n",
    "\n",
    "# Evaluate models\n",
    "if \"label\" in new_df.columns:  # If ground truth labels are available\n",
    "    results = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        y_pred = model.predict(X_new_pca)\n",
    "        acc = accuracy_score(y_new, y_pred)\n",
    "        precision = precision_score(y_new, y_pred)\n",
    "        recall = recall_score(y_new, y_pred)\n",
    "        f1 = f1_score(y_new, y_pred)\n",
    "\n",
    "        results.append([name, acc, precision, recall, f1])\n",
    "\n",
    "    results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "    print(results_df)\n",
    "else:\n",
    "    # Predict only (if no labels are available)\n",
    "    predictions = {name: model.predict(X_new_pca) for name, model in models.items()}\n",
    "    for name, pred in predictions.items():\n",
    "        print(f\"Predictions for {name}:\", pred[:10])  # Show first 10 predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
